<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>爬虫 on 我爱ETF</title><link>http://52etf.net/tags/%E7%88%AC%E8%99%AB/</link><description>Recent content in 爬虫 on 我爱ETF</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Mon, 15 Jun 2020 00:00:00 +0000</lastBuildDate><atom:link href="http://52etf.net/tags/%E7%88%AC%E8%99%AB/index.xml" rel="self" type="application/rss+xml"/><item><title>抓取微信公众号内容</title><link>http://52etf.net/blog/%E6%8A%93%E5%8F%96%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7/</link><pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate><guid>http://52etf.net/blog/%E6%8A%93%E5%8F%96%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7/</guid><description>导语 我经常会集中阅读一些公众号内容，手机上看效率非常低，所以要将公众号整理成册子。 无论是手机还是电脑版的公众号，都对爬虫做了非常强的限制，主要体现在页面内容地址是算法生成的，没有固定的模式，还有过期限</description></item></channel></rss>